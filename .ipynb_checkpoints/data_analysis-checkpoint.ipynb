{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "rental-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import folium\n",
    "from folium import FeatureGroup, LayerControl, Map, Marker\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy\n",
    "#!pip install folium\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-exemption",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "impressive-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Location_Tree.csv', 'Test_rev1.csv', 'Train_rev1.csv', 'Valid_rev1.csv', 'mean_benchmark.csv', 'random_forest_benchmark_test_rev1.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get files from GCS bucket\n",
    "\n",
    "BUCKET_NAME = 'salary-data'\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "blobs = bucket.list_blobs()\n",
    "files = []\n",
    "for blob in blobs:\n",
    "    files.append(blob.name)\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "efficient-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train set from bucket\n",
    "train_set_name = files[2]\n",
    "\n",
    "df_train = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,train_set_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,files[4]))\n",
    "df_location_tree = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,files[0]))\n",
    "df_rf = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,files[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "#df_location_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-demographic",
   "metadata": {},
   "source": [
    "# Visualise and analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-roman",
   "metadata": {},
   "source": [
    "## Check salary distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_train.hist(column = 'SalaryNormalized', bins = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-soundtrack",
   "metadata": {},
   "source": [
    "## Check for any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "\n",
    "df_train.isnull().sum()\n",
    "#df_train.isna().sum()\n",
    "\n",
    "#df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-validity",
   "metadata": {},
   "source": [
    "## Check the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cities = df_train.LocationRaw.unique()\n",
    "norm_cities = df_train.LocationNormalized.unique()\n",
    "\n",
    "print('Number of unique raw locations = {}'.format(len(raw_cities)))\n",
    "print('Number of unique normalised locations = {}'.format(len(norm_cities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_counts = df_train.LocationRaw.value_counts()\n",
    "norm_counts = df_train.LocationNormalized.value_counts()\n",
    "\n",
    "print('Top raw locations \\n')\n",
    "print(raw_counts[0:10] )\n",
    "print('\\n')\n",
    "print('Top normalised locations \\n')\n",
    "print(norm_counts[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent='myapplication')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = geolocator.geocode(\"Surrey\")\n",
    "coordinates = [location.latitude,location.longitude]\n",
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_map = folium.Map(location = [51.50,-0.13],zoom_start = 5,)\n",
    "\n",
    "folium.CircleMarker(coordinates,\n",
    "                    radius = 1.5,                    \n",
    "                    color = 'red', \n",
    "                    fill_opacity=0.5\n",
    "                   ).add_to(drop_map)\n",
    "drop_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-secondary",
   "metadata": {},
   "source": [
    "## Check the Job titles/ company/ category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_title = df_train.Title.unique()\n",
    "uni_cat = df_train.Category.unique()\n",
    "uni_comp = df_train.Company.unique()\n",
    "\n",
    "print('Number of unique titles= {}'.format(len(uni_title)))\n",
    "print('Number of unique categories = {}'.format(len(uni_cat)))\n",
    "print('Number of unique companies = {}'.format(len(uni_comp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts = df_train.Title.value_counts()\n",
    "category_counts = df_train.Category.value_counts()\n",
    "company_counts = df_train.Company.value_counts()\n",
    "\n",
    "# norm_counts = df_train.LocationNormalized.value_counts()\n",
    "\n",
    "\n",
    "print('Top titles \\n')\n",
    "print(title_counts[0:10] )\n",
    "print('\\n')\n",
    "print('Top categories \\n')\n",
    "print(category_counts[0:10])\n",
    "print('\\n')\n",
    "print('Top companies \\n')\n",
    "print(company_counts[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-chorus",
   "metadata": {},
   "source": [
    "## Check the difference between contract time and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_contract = df_train.ContractType.unique()\n",
    "time_contract = df_train.ContractTime.unique()\n",
    "\n",
    "print(type_contract)\n",
    "print(time_contract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nans and check if there is a difference in average salary between contracts\n",
    "\n",
    "df_no_nan_type = df_train[df_train.ContractType.notna()]\n",
    "df_no_nan_time = df_train[df_train.ContractTime.notna()]\n",
    "\n",
    "df_avg_type = df_no_nan_type.groupby(['ContractType']).mean()\n",
    "df_avg_time = df_no_nan_time.groupby(['ContractTime']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_type = df_avg_type.reset_index()\n",
    "df_avg_time = df_avg_time.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "df_avg_type.plot.bar(x ='ContractType',y='SalaryNormalized', ax = axes[0])\n",
    "\n",
    "df_avg_time.plot.bar(x ='ContractTime',y='SalaryNormalized', ax = axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-henry",
   "metadata": {},
   "source": [
    " No big difference between contract or permanent. There does seem to be a difference between full time and partime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-front",
   "metadata": {},
   "source": [
    "## Filling in NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.ContractType = df_train.ContractType.fillna('Missing')\n",
    "df_train.ContractTime = df_train.ContractTime.fillna('Missing')\n",
    "df_train.Company = df_train.Company.fillna('Missing')\n",
    "df_train = df_train.dropna()\n",
    "df_train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-fundamentals",
   "metadata": {},
   "source": [
    "# NLP of the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('k')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-desperate",
   "metadata": {},
   "source": [
    "## Define functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fitting-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(df_train, size, split):\n",
    "    \n",
    "    train_size =slice( 0,round(size*split))\n",
    "    test_size = slice(round(size*split)+1 ,size+1)\n",
    "\n",
    "    description = df_train.FullDescription\n",
    "    Salary = df_train.SalaryNormalized\n",
    "\n",
    "    train = description[train_size]\n",
    "    test = description[test_size]\n",
    "\n",
    "    y_train = Salary[train_size]\n",
    "    y_test = Salary[test_size]\n",
    "    \n",
    "    return( train, test , y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "def remove_stop_words(bag, unique):\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_tokens = tokenizer.tokenize(bag)\n",
    "    \n",
    "    words = []\n",
    "    for word in word_tokens:\n",
    "        words.append(word.lower())\n",
    "    # Get unique words only\n",
    "    if unique:\n",
    "        words_set = set(words)\n",
    "        words = list(words_set)\n",
    "    \n",
    "    no_stop_words = [w for w in words if not w in stop_words] \n",
    "    \n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the words  \n",
    "\n",
    "def lemmatize_words(no_stop_words):\n",
    "    \n",
    "    lemma = WordNetLemmatizer() \n",
    "    lemma_words = []\n",
    "\n",
    "    for w in no_stop_words:\n",
    "        lemma_words.append(lemma.lemmatize(w))\n",
    "    \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(train):\n",
    "    vocab = []\n",
    "    for index, row in train.items():\n",
    "\n",
    "        no_stop_words = remove_stop_words(row, unique=False)\n",
    "        lemma_words = lemmatize_words(no_stop_words)\n",
    "\n",
    "        vocab.append(lemma_words)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "faced-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(X_train, X_test):\n",
    "    lengths =[]\n",
    "    for x in X_test:\n",
    "        lengths.append(len(x))\n",
    "    for x in X_train:\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "lesser-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(train,test): \n",
    "    # Preprocess train set and make a vocabulary\n",
    "    vocab = create_vocab(train)\n",
    "\n",
    "    # create dictionary of text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "    # Preprocess train and test set\n",
    "    pre_train = create_vocab(train)\n",
    "    pre_test = create_vocab(test)\n",
    "\n",
    "    # Convert text to indexed list\n",
    "    X_train = tokenizer.texts_to_sequences(pre_train)\n",
    "    X_test = tokenizer.texts_to_sequences(pre_test)\n",
    "    vocab = tokenizer.word_index\n",
    "    \n",
    "    return X_train, X_test, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "advisory-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(*argv): # X_train, X_test\n",
    "    if len(argv) == 1:\n",
    "        X_train = argv[0]\n",
    "        \n",
    "        lengths =[]\n",
    "        for x in X_train:\n",
    "            lengths.append(len(x))\n",
    "            \n",
    "        max_length = max(lengths)\n",
    "            \n",
    "        X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train,maxlen=max_length, padding='post')\n",
    "        \n",
    "        return X_train_padded\n",
    "      \n",
    "    else:\n",
    "        X_train = argv[0]\n",
    "        X_test = argv[1]\n",
    "        max_length = get_max_length(X_train, X_test)\n",
    "        \n",
    "        X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train,maxlen=max_length, padding='post')\n",
    "        X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "        \n",
    "        return X_train_padded, X_test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-hunter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bacterial-christian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "neural-beatles",
   "metadata": {},
   "source": [
    "## Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "colored-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "\n",
    "size = 100\n",
    "split = 0.8\n",
    "\n",
    "train, test, y_train, y_test = make_train_test(df_train, size, split)\n",
    "\n",
    "X_train, X_test, vocab = prep_data(train,test)\n",
    "\n",
    "X_train_padded, X_test_padded = padding(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-durham",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "binding-diagnosis",
   "metadata": {},
   "source": [
    "## Prepare target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "exact-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on whether salary is > 25000 or not\n",
    "\n",
    "y_binary_train = y_train < 25000\n",
    "y_binary_train = (y_binary_train*1).values\n",
    "\n",
    "y_binary_test = y_test < 25000\n",
    "y_binary_test = (y_binary_test*1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "qualified-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio is 0.4\n",
      "Test ratio is 0.65\n"
     ]
    }
   ],
   "source": [
    "# Check ratio of 0s and 1s\n",
    "\n",
    "print('Train ratio is {}'.format(sum(y_binary_train)/len(y_binary_train)))\n",
    "print('Test ratio is {}'.format(sum(y_binary_test)/len(y_binary_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-central",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "stock-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "max_len = X_train_padded.shape[1]\n",
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "confused-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 390, 8)            17672     \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3120)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 3121      \n",
      "=================================================================\n",
      "Total params: 20,793\n",
      "Trainable params: 20,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-establishment",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "electronic-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train_padded, y_binary_train, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_binary_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-plenty",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "legitimate-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "identical-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "split = 1  # use a k-fold cross validation instead of a simple split\n",
    "\n",
    "train, test, y_train, y_test = make_train_test(df_train, size, split)\n",
    "\n",
    "X_train = prep_data(train)\n",
    "\n",
    "X_train_padded = padding(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-limitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model(vocab_size = vocab_size,max_len = max_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 8, input_length=max_len))\n",
    "    model.add(Flatten())\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=regression_model, epochs=50, batch_size=1, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-maldives",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-offer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-absence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-dallas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-prague",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-palace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "treated-sleeve",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-nashville",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
