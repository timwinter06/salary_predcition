{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "falling-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cognitive-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2011_Travel_to_Work_Areas_summary_statistics_V5.csv', 'Location_Tree.csv', 'Test_rev1.csv', 'Train_rev1.csv', 'Valid_rev1.csv', 'X_test_padded', 'X_train_padded', 'X_val_padded', 'data_location_buckets.csv', 'mean_benchmark.csv', 'random_forest_benchmark_test_rev1.csv', 'test.csv', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "# Get files from GCS bucket\n",
    "BUCKET_NAME = 'salary-data'\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "blobs = bucket.list_blobs()\n",
    "files = []\n",
    "for blob in blobs:\n",
    "    files.append(blob.name)\n",
    "    \n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "funky-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_TTWA = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,files[0]))\n",
    "df_train = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,files[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-carter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_6 import TTWA_county_feature\n",
    "df_loc = TTWA_county_feature(df_train,df_TTWA,True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "TTWA_county = df_loc.TTWA_County\n",
    "df_all = pd.concat([df_train, TTWA_county], axis = 1)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "bucket.blob('data_location_buckets.csv').upload_from_string(df_all.to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-syria",
   "metadata": {},
   "source": [
    "## Deal with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "provincial-oregon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                    0\n",
       "Title                 0\n",
       "FullDescription       0\n",
       "LocationRaw           0\n",
       "LocationNormalized    0\n",
       "ContractType          0\n",
       "ContractTime          0\n",
       "Company               0\n",
       "Category              0\n",
       "SalaryRaw             0\n",
       "SalaryNormalized      0\n",
       "SourceName            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing values for Contract and Company\n",
    "df_all =df_train\n",
    "df_all.ContractType = df_train.ContractType.fillna('Missing')\n",
    "df_all.ContractTime = df_train.ContractTime.fillna('Missing')\n",
    "df_all.Company = df_train.Company.fillna('Missing')\n",
    "# Drop other NaN values\n",
    "df_all = df_all.dropna()\n",
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-scheme",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aging-wrestling",
   "metadata": {},
   "source": [
    "## NLP of Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brief-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_12 as fn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "twelve-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "egyptian-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_NLP(df):\n",
    "    word_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        bag = row.FullDescription\n",
    "        ns = fn.remove_stop_words(bag, unique = False)\n",
    "        lemma = fn.lemmatize_words(ns)\n",
    "        word_list.append(lemma)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blind-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train, val, test split\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                                df_train.drop(['SalaryNormalized','SalaryRaw'], axis =1),\n",
    "                                df_train.SalaryNormalized, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                                                X_train_full,\n",
    "                                                y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acceptable-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove grammer, capitals, and stop words\n",
    "X_train_words = first_NLP(X_train)\n",
    "X_val_words = first_NLP(X_val)\n",
    "X_test_words = first_NLP(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-observation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binding-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of only the TRAIN text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_words)\n",
    "word_dictionary = tokenizer.word_index\n",
    "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to indexed list\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_words)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val_words)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "greenhouse-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "\n",
    "max_length = fn.get_max_length(X_train_seq, X_test_seq, X_val_seq)\n",
    "\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq,maxlen=max_length, padding='post')\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq,maxlen=max_length, padding='post')\n",
    "X_val_padded = tf.keras.preprocessing.sequence.pad_sequences(X_val_seq,maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-thanks",
   "metadata": {},
   "source": [
    "## Save/ Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tight-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = tokenizer.word_index\n",
    "file = './word_index.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, 'w') as f: \n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "juvenile-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "floppy-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_X_train =  pd.DataFrame(X_train_padded)\n",
    "# df_X_test = pd.DataFrame(X_test_padded)\n",
    "# df_X_val = pd.DataFrame(X_val_padded)\n",
    "# df_size_vocab = pd.DataFrame([size_of_vocabulary])\n",
    "\n",
    "# bucket.blob('X_train_padded').upload_from_string(df_X_train.to_csv())\n",
    "# bucket.blob('X_test_padded').upload_from_string(df_X_test.to_csv())\n",
    "# bucket.blob('X_val_padded').upload_from_string(df_X_val.to_csv())\n",
    "# bucket.blob('vocab_size').upload_from_string(df_size_vocab.to_csv())\n",
    "\n",
    "df_X_train = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,'X_train_padded'))\n",
    "df_X_test = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,'X_test_padded'))\n",
    "df_X_val = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,'X_val_padded'))\n",
    "df_size_vocab = pd.read_csv('gs://{}/{}'.format(BUCKET_NAME,'vocab_size'))\n",
    "\n",
    "X_train_padded = df_X_train.to_numpy()[:,1:]\n",
    "X_test_padded = df_X_test.to_numpy()[:,1:]\n",
    "X_val_padded = df_X_val.to_numpy()[:,1:]\n",
    "size_of_vocabulary = df_size_vocab.to_numpy()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "conscious-leonard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-petroleum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-hunter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eligible-spank",
   "metadata": {},
   "source": [
    "## Load the GloVe embeddings and make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "recorded-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word embeddings from GloVe\n",
    "glove_model = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "excess-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index[word] = dict()\n",
    "glove_dict = glove_model.vocab\n",
    "\n",
    "for key, value in glove_dict.items():\n",
    "    word = key\n",
    "    coef = glove_model[key]\n",
    "    embeddings_index[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "intermediate-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glove_model.vectors[value.index]) \n",
    "# print(glove_model[key]) # array\n",
    "# print(key)# word\n",
    "# print(value.index) # \n",
    "# print(embeddings_index['i'])\n",
    "# print(glove_model['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-reception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "constant-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((size_of_vocabulary, 25))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-feature",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "noble-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "backed-gateway",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6895b4761a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_of_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.add(layers.Dense(10, activation = 'relu'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model =Sequential()\n",
    "model.add(layers.Embedding(size_of_vocabulary, 25, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(10, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-ticket",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-helicopter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oriental-medication",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-082710aa2e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit(X_train_padded, y_train.values, epochs=10, verbose=0,\n\u001b[0m\u001b[1;32m      3\u001b[0m          validation_data=(X_val_padded, y_val.values))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train_padded, y_train.values, epochs=10, verbose=0,\n",
    "         validation_data=(X_val_padded, y_val.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-leather",
   "metadata": {},
   "source": [
    "## Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss = model.evaluate(X_test_padded, y_val.values, verbose=0)\n",
    "print('MSE: {}'.format(loss))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
